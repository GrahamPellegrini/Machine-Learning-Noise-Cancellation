{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded886f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv-TasNet Hyperparameter Tuning Notebook\n",
    "# Author: Graham Pellegrini | UOM Final Year Project\n",
    "\n",
    "# ============================\n",
    "# üì¶ 1. Setup & Imports\n",
    "# ============================\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from Utils.train import train_eval\n",
    "from Utils.models import ConvTasNet\n",
    "from Utils.dataset import DynamicBuckets, BucketSampler\n",
    "import config\n",
    "\n",
    "# ============================\n",
    "# üß© 2. Configuration\n",
    "# ============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset and preprocessing parameters\n",
    "sr = config.SAMPLE_RATE\n",
    "n_fft = config.N_FFT\n",
    "hop_length = config.HOP_LENGTH\n",
    "batch_size = config.BATCH_SIZE\n",
    "accum_steps = config.ACCUMULATION_STEPS\n",
    "num_workers = config.NUM_WORKERS\n",
    "num_buckets = config.NUM_BUCKET\n",
    "\n",
    "# Datasets\n",
    "train_dataset = DynamicBuckets(config.DATASET_DIR, \"trainset_56spk\", sr, n_fft, hop_length, num_buckets)\n",
    "val_dataset = DynamicBuckets(config.DATASET_DIR, \"trainset_28spk\", sr, n_fft, hop_length, num_buckets)\n",
    "train_sampler = BucketSampler(train_dataset.bucket_indices, batch_size=batch_size)\n",
    "val_sampler = BucketSampler(val_dataset.bucket_indices, batch_size=batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_sampler, num_workers=num_workers)\n",
    "\n",
    "# ============================\n",
    "# üîç 3. Define Search Space\n",
    "# ============================\n",
    "hparam_trials = [\n",
    "    {\"enc_dim\": 64, \"feature_dim\": 32, \"kernel_size\": (3, 3), \"num_layers\": 3, \"num_stacks\": 2},\n",
    "    {\"enc_dim\": 128, \"feature_dim\": 48, \"kernel_size\": (3, 3), \"num_layers\": 4, \"num_stacks\": 2},  # baseline\n",
    "    {\"enc_dim\": 128, \"feature_dim\": 64, \"kernel_size\": (5, 5), \"num_layers\": 4, \"num_stacks\": 3},\n",
    "    {\"enc_dim\": 192, \"feature_dim\": 64, \"kernel_size\": (3, 3), \"num_layers\": 5, \"num_stacks\": 2},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# ============================\n",
    "# üöÄ 4. Run Trials\n",
    "# ============================\n",
    "for i, hparams in enumerate(hparam_trials):\n",
    "    print(f\"\\nüéØ Trial {i+1}: {hparams}\")\n",
    "\n",
    "    model = ConvTasNet(\n",
    "        enc_dim=hparams[\"enc_dim\"],\n",
    "        feature_dim=hparams[\"feature_dim\"],\n",
    "        kernel_size=hparams[\"kernel_size\"],\n",
    "        num_layers=hparams[\"num_layers\"],\n",
    "        num_stacks=hparams[\"num_stacks\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    save_path = f\"Models/Trial_{i+1}_ConvTasNet.pth\"\n",
    "    \n",
    "    start = time.time()\n",
    "    train_eval(\n",
    "        device,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        epochs=5,  # fewer epochs for tuning\n",
    "        accumulation_steps=accum_steps,\n",
    "        save_pth=save_path,\n",
    "        pto=False,\n",
    "        scheduler=config.SCHEDULER\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    # Record result\n",
    "    results.append({\n",
    "        \"trial\": i+1,\n",
    "        **hparams,\n",
    "        \"val_loss\": model.best_val_loss if hasattr(model, \"best_val_loss\") else \"NA\",\n",
    "        \"time\": end - start\n",
    "    })\n",
    "\n",
    "# ============================\n",
    "# üìä 5. Save Results & Plot\n",
    "# ============================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"Output/hparam_tuning_results.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(results_df[\"trial\"], results_df[\"val_loss\"], tick_label=[f\"T{i+1}\" for i in range(len(results_df))])\n",
    "plt.title(\"Conv-TasNet Validation Loss per Trial\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Output/png/hparam_val_loss_plot.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
