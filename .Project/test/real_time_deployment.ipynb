{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from Utils.models import ConvTasNet  # adjust if your path differs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953608d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio settings\n",
    "SAMPLE_RATE = 48000\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "DURATION = 3  # seconds of live audio capture\n",
    "\n",
    "# Model path\n",
    "MODEL_PATH = \".Project/Models/ConvTasNet_dynamic.pth\"\n",
    "OUTPUT_PATH = \"Output/wav/real_time_denoised.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51665a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model = ConvTasNet()\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define transforms\n",
    "spectrogram = T.Spectrogram(n_fft=N_FFT, hop_length=HOP_LENGTH, power=None).to(device)\n",
    "inverse_spectrogram = T.InverseSpectrogram(n_fft=N_FFT, hop_length=HOP_LENGTH).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üé§ Recording {DURATION} seconds of audio...\")\n",
    "recording = sd.rec(int(DURATION * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='float32')\n",
    "sd.wait()\n",
    "print(\"‚úÖ Recording complete.\")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "noisy_waveform = torch.tensor(recording.T, dtype=torch.float32).to(device)  # shape: (1, samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to spectrogram\n",
    "noisy_spec = spectrogram(noisy_waveform)\n",
    "\n",
    "# Normalize\n",
    "noisy_spec = (noisy_spec - noisy_spec.mean()) / (noisy_spec.std() + 1e-6)\n",
    "\n",
    "# Split real and imaginary\n",
    "noisy_real, noisy_imag = noisy_spec.real, noisy_spec.imag\n",
    "\n",
    "# Add channel dim\n",
    "noisy_real = noisy_real.unsqueeze(1)\n",
    "noisy_imag = noisy_imag.unsqueeze(1)\n",
    "\n",
    "# Inference and timing\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    denoised_real, denoised_imag = model(noisy_real, noisy_imag)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "print(f\"‚è±Ô∏è Inference time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6491c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back to complex\n",
    "denoised_spec = torch.complex(denoised_real.squeeze(1), denoised_imag.squeeze(1))\n",
    "\n",
    "# Convert to waveform\n",
    "denoised_waveform = inverse_spectrogram(denoised_spec)\n",
    "\n",
    "# Clamp to avoid clipping\n",
    "denoised_waveform = torch.clamp(denoised_waveform, min=-1.0, max=1.0)\n",
    "\n",
    "# Save as WAV\n",
    "write(OUTPUT_PATH, SAMPLE_RATE, denoised_waveform.cpu().numpy().T.astype(np.float32))\n",
    "print(f\"‚úÖ Denoised audio saved at: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ac2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Justification: frame processing rate\n",
    "print(f\"üìà Real-time factor (RTF): {inference_time:.4f} / {DURATION:.2f} = {inference_time / DURATION:.4f}\")\n",
    "if inference_time < DURATION:\n",
    "    print(\"‚úÖ This implementation qualifies for real-time deployment.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è May not be real-time. Consider reducing model complexity or batching.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnc-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
