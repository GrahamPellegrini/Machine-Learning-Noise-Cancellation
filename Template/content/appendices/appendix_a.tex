\chapter{Project Structure}
\label{appendix:project_structure}

The overall structure of the project is modular, enabling clean separation of concerns and supporting future extensibility. The project is organised around two main pipelines \textbf{training} and \textbf{denoising}. Each pipeline is implemented as a collection of dedicated modules, coordinated through the central \texttt{main.py} script, which serves as the entry point for running the system.

To maintain clarity and modularity, all helper modules are organised within a \textit{Utils} directory. This directory contains the core functionality for dataset handling, \gls{ml} model definitions, training routines, inference logic, and classical baseline methods. Additionally, a centralised configuration file, \texttt{config.py}, located alongside \texttt{main.py}, manages all project parameters using a dictionary-based structure. This design allows users to easily modify hyperparameters or experiment settings in a single location, improving usability and reproducibility.

An overview of the key files in the \textit{Utils} directory is provided below:

\begin{itemize}
    \item \texttt{dataset.py}: Contains all dataset classes used in the project, implemented as PyTorch \texttt{Dataset} objects. The file supports multiple strategies for handling variable-length audio inputs, as discussed in Section~\ref{sec:datasets}. Along with the neccesary helper functions.

    \item \texttt{model.py}: Defines the \gls{ml} model architectures used for speech enhancement. Each model is implemented as a subclass of PyTorchâ€™s \texttt{nn.Module}, and the module is designed for easy experimentation with new architectures or changes in hyperparameters. Many implementations had to be adapted from the original papers and the desing of such changes is discussed in Section~\ref{sec:model_architecture}.

    \item \texttt{train.py}: Implements the training loop, validation logic, and performance tracking. It supports training using any of the dataset classes defined in \texttt{dataset.py}. It also includes the \gls{oom} handeling disucssed in Section~\ref{sec:oom_handling} and justified in Section~\ref{sec:oom_validation}

    \item \texttt{denoise.py}: Handles inference and evaluation post-training. It defines two core functions: \texttt{batch\_denoise}, for full-dataset evaluation, and \texttt{single\_denoise}, for individual files. The module supports both \gls{ml}-based and classical methods, enabling seamless switching between approaches. It also includes \texttt{compute\_metrics}, which calculates objective metrics such as \gls{snr}, \gls{mse}, \gls{lsd}, \gls{pesq}, and \gls{stoi}, with all except \gls{lsd} computed using library functions. Metric calculations are consistent throughout the project. Classical methods, including \gls{ss}, \gls{wf}, and \gls{mmse-lsa}, are self-implemented to support spectrogram-based processing.

\end{itemize}
