\chapter{Project Structure}
\label{appendix:project_structure}

The overall structure of the project is modular, enabling clean separation of concerns and supporting future extensibility. The project is organised around two main pipelines \textbf{training} and \textbf{denoising}. Each pipeline is implemented as a collection of dedicated modules, coordinated through the central \texttt{main.py} script, which serves as the entry point for running the system.

To maintain clarity and modularity, all helper modules are organised within a \textit{Utils} directory. This directory contains the core functionality for dataset handling, \gls{ml} model definitions, training routines, inference logic, and classical baseline methods. Additionally, a centralised configuration file, \texttt{config.py}, located alongside \texttt{main.py}, manages all project parameters using a dictionary-based structure. This design allows users to easily modify hyperparameters or experiment settings in a single location, improving usability and reproducibility.

An overview of the key files in the \textit{Utils} directory is provided below:

\begin{itemize}
    \item \texttt{dataset.py}: Contains all dataset classes used in the project, implemented as PyTorch \texttt{Dataset} objects. The file supports multiple strategies for handling variable-length audio inputs, as discussed in Section~\ref{sec:datasets}. Along with the neccesary helper functions.

    \item \texttt{model.py}: Defines the \gls{ml} model architectures used for speech enhancement. Each model is implemented as a subclass of PyTorchâ€™s \texttt{nn.Module}, and the module is designed for easy experimentation with new architectures or changes in hyperparameters. Many implementation had to be adapted from the original paper and the desing of such changes is discussed in Section~\ref{sec:model_architecture}.

    \item \texttt{train.py}: Implements the training loop, validation logic, and performance tracking. It supports training using any of the dataset classes defined in \texttt{dataset.py}. It also includes the \gls{oom} handeling disucssed in Section~\ref{sec:oom_handling} and justified in Section~\ref{sec:oom_validation}

    \item \texttt{denoise.py}: Handles inference and evaluation after training. This module includes two core functions: \texttt{batch\_denoise} for evaluating entire datasets and \texttt{single\_denoise} for individual file processing. It supports both \gls{ml}-based models and classical denoising algorithms, allowing seamless switching between the two approaches. The file also defines the \texttt{compute\_metrics} function, which calculates objective evaluation metrics such as \gls{snr}, \gls{mse}, \gls{lsd}, \gls{pesq}, and \gls{stoi}. All but \gls{lsd} are calculated using libary function. Furthermore, metric calcualtions are consistent throughout the whole project. Classical methods such as \gls{ss}, \gls{wf}, and \gls{mmse-lsa} are self implemented bsince there methods need to be adapted for spectrogram data.
\end{itemize}
