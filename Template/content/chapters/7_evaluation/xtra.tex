


Following the dataset analysis, the same three CED models are evaluated on denoising performance using the objective metrics discussed in Chapter~\ref{chp:implementation}. This comparison highlights how variable-length handling strategies can influence final model output, despite having otherwise identical training configurations.


The efficiency of variable-length audio handling strategies was measured by tracking the time taken to load and process each dataset during training. Table~\ref{tab:dataset_loading_times} summarizes the total preprocessing and loading times for the Static, Dynamic, and PTO datasets across both cached and uncached runs. The PTO dataset additionally incurs a non-negligible truncation time during each training epoch due to the need to crop padded outputs based on original sequence lengths.



The results show that:
\begin{itemize}
    \item \textbf{Dynamic Bucketing} has the longest uncached loading time due to the K-Means clustering procedure used to determine bucket centers. However, it benefits significantly from caching on subsequent runs.
    \item \textbf{Static Bucketing} offers the fastest preprocessing and reload times overall, owing to its predefined structure and simple assignment mechanism.
    \item \textbf{PTO} is relatively quick to initialize, but incurs per-epoch runtime overhead due to the need to truncate model outputs during training.
\end{itemize}

In summary, while caching significantly improves all methods, the PTO approach introduces unique runtime costs, and the Dynamic Bucketing approach is the most expensive to compute initially. These results are crucial in environments with constrained computational resources and help inform the choice of dataset preprocessing strategy depending on usage constraints.
