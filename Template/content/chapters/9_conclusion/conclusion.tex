\chapter{Conclusion}
\label{chap:conclusion}

This project set out to explore and compare classical and \gls{ml} approaches for speech enhancement. A modular pipeline was implemented, supporting flexible experimentation through custom dataset loaders, real-time inference support, and unified evaluation metrics. Critical implementation flaws, such as the improper use of \texttt{Tanh} activations, were identified and resolved through iterative metric-based and visual analysis.

Evaluation began with classical methods: \gls{ss}, \gls{wf}, and \gls{mmse-lsa}, establishing a reliable performance baseline. The project then introduced and assessed five \gls{ml} modelsâ€”\gls{cnn}, \gls{ced}, \gls{rced}, \gls{unet}, and \gls{convtasnet}. All trained from scratch in the spectrogram domain using the Edinburgh DataShare dataset. A key contribution was the design and evaluation of variable-length dataset handling strategies, including static and dynamic bucketing and padding-truncation approaches. These ensured efficient training across sequences of diverse lengths.

To address hardware limits, \gls{oom} mitigation strategies were implemented. While these slightly reduced training accuracy, they consistently improved denoising metrics and enabled deeper models like \gls{unet} and \gls{convtasnet} to be trained successfully. \gls{convtasnet} emerged as the top performer, achieving an \gls{snr} of 18.06~dB, \gls{pesq} of 2.43, and \gls{stoi} of 0.91. Although training was computationally intensive, ConvTasNet took over 25 hours, these were one time offline costs. Inference remained efficient, with the entire test set denoised in under two minutes.

In conclusion, this project demonstrated the viability of training speech enhancement models from scratch using a well-engineered system. The final pipeline supports robust evaluation and real-time denoising, validating \gls{ml}-based approaches as a practical and superior alternative to classical methods. This work lays the foundation for potential future developments Chapter~\ref{chap:future_work}.
