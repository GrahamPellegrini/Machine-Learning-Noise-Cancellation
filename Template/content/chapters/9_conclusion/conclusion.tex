\chapter{Conclusion}
\label{chap:conclusion}

This project set out to explore and compare classical and machine learning approaches for speech enhancement.Thorough work has been done on modular architecture, the integration of custom dataset loaders, inference pipelines, and evaluation metrics, so that the system is able to support both experimentation and real-world adaptability. Out-of-memory mitigation strategies enabled the training of deeper models under hardware constraints. Key design flaws such as the improper use of \texttt{Tanh} activations, were identified and corrected through iterative visual and metric based analysis.

The evlaution began with classical signal processing techniques. Spectral Subtraction and Wiener Filtering were implemented first, followed by the integration of MMSE-LSA as a more recent and perceptually motivated method. These approaches established a meaningful benchmark and provided valuable insights into the trade-offs between numerical fidelity and perceptual quality.

The core contribution of this project lies in the implementation and evaluation of five machine learning models: CNN, CED, RCED, UNet, and ConvTasNet. These models were trained entirely from scratch on the Edinburgh DataShare speech dataset and adapted to operate in the spectrogram domain. Significant care was taken to address practical challenges such as variable-length input handling, memory limitations, and output scaling. Evaluations showed a consistent upward trend in performance, with each successive architecture improving on both energy-based and perceptual metrics. ConvTasNet ultimately emerged as the highest-performing model, achieving an SNR of 18.06 dB, PESQ of 2.43, and STOI of 0.91, surpassing both other machine learning models and all classical baselines.

While deep learning models delivered strong denoising results, these came at the cost of increased training time. In our experiments, ConvTasNet required over 25 hours to train, with UNet taking over 17 hours. However, these are one-time offline costs. Once trained, all models demonstrated efficient inference, with denoising of the entire test set averaging around one and a half minutes. This confirms the systemâ€™s viability for real-time applications and reinforces the practicality of deploying deep learning models in live speech enhancement settings.

In addition to the main evaluation, extended experiments in Appendix~\ref{sec:pretrained_comparison} showed that while our ConvTasNet performed competitively on in-domain data, it struggled to generalize to foreign noise conditions. Pretrained models such as \texttt{DeepFilterNet}, trained on broader datasets, exhibited stronger cross-domain robustness. These findings underscore the importance of training scale and dataset diversity in achieving real-world generalization.

In conclusion, this project has demonstrated the feasibility and effectiveness of training speech enhancement models from scratch using a carefully engineered pipeline. The final system provides not only a strong comparative analysis but also a robust foundation for continued research and deployment. Whether through the integration of more advanced architectures, expansion to multichannel audio, or further optimization for embedded real-time systems, this work lays the groundwork. Justifing machine learning techniques as the way foward over classical methods for speech enhancement.
