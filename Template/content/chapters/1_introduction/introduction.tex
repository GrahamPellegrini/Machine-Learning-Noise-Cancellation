\graphicspath{{content/chapters/1_introduction/figures}}
\chapter{Introduction}
\label{chp:introduction}

\gls{ml}, commonly referred to as \gls{ai}, has seen rapid advancements over the past decade, largely driven by increasing computational power and the availability of large datasets. These techniques are now widely applied to solve complex problems, including speech and noise processing. However, the adoption of \gls{ai} in such areas often lacks thorough evaluation, with the justifications for their computational demands not always clearly addressed.

This project focuses on noise cancellation in audio signals for speech enhancement. An important area in telecommunications, assistive technologies, and real-time communication. Background noise can significantly degrade speech quality, making it harder to interpret or process spoken information. Such noise is generally categorized into two types: stationary and non-stationary \cite{loizou2013speech}.

\begin{itemize}
    \item \textbf{Stationary noise} has relatively constant properties over time, such as white noise or the hum of an appliance.
    \item \textbf{Non-stationary noise} fluctuates unpredictably, such as traffic, crowd chatter, or sudden environmental sounds.
\end{itemize}

Speech enhancement aims to suppress these unwanted noise components while preserving speech intelligibility. Classical noise cancellation techniques, such as the Wiener filter, estimate and subtract noise from the signal but often assume stationarity. This assumption limits their effectiveness in real-world conditions. In contrast, \gls{ml} approaches have emerged as powerful alternatives. They can learn complex data-driven patterns to separate speech from noise without relying on such rigid assumptions.

\section{Project Goals and Implementation}

The primary goal of the project is to explore both classical and \gls{ml} based approaches to noise cancellation, evaluating their effectiveness and feasibility in real-world applications. In addition to examining established techniques, a custom \gls{ml} model is developed and compared against classical methods.

The system assumes a single-speaker scenario with background noise, requiring the removal of both stationary and non-stationary noise without access to a clean reference. Unlike established pre-trained models that require extensive datasets and resources, the model developed in this project will not be pre-trained. This demonstrates ease of development and training in practical settings. Pre-trained models will be used as part of the evaluation process, but the focus will be on the model developed in this project.

The project will be implemented in Python using a modular and reproducible structure, ensuring the framework can be easily extended or modified. The project comprises two core phases:

\begin{itemize}
    \item Training phase: The most computationally intensive stage, using clean and noisy speech to train the model for noise removal.
    \item Denoising phase: Involves loading the trained model and applying it to a noisy speech signal to generate a cleaned output. Evaluation metrics are computed to compare method performance.
\end{itemize}

\section{Overview of Contents}

The remainder of this report is structured as follows:
\begin{itemize}
    \item \textbf{Chapter~\ref{chp:background}} presents the foundational theory behind speech enhancement. It introduces classical noise reduction techniques: Spectral Subtraction, Wiener Filtering, and MMSE-LSA. As well as the principles of autoencoder-based \gls{ml} models and the evaluation metrics used.

    \item \textbf{Chapter~\ref{chp:literature_review}} reviews the key literature that informed the system design. It focuses on handling variable-length audio, the application of Fully Convolutional Networks (FCNs), and the Conv-TasNet architecture.

    \item \textbf{Chapter~\ref{chp:specification}} outlines the project requirements, including dataset structure, environment setup, and hardware constraints. These informed the design.

    \item \textbf{Chapter~\ref{chp:design}} describes the system architecture, addressing how variable-length audio is handled during training and detailing the rationale for each \gls{ml} model.

    \item \textbf{Chapter~\ref{chp:implementation}} explains the technical implementation, highlighting custom dataset components and memory optimisation during training.

    \item \textbf{Chapter~\ref{chp:evaluation}} presents a detailed evaluation of all methods. It compares dataset strategies, validates out-of-memory techniques, and benchmarks five \gls{ml} models against three classical baselines using objective and subjective metrics.

    \item \textbf{Chapter~\ref{chp:future_work}} outlines directions for extending the work, including transformer- and diffusion-based models.

    \item \textbf{Chapter~\ref{chp:conclusion}} summarises the outcomes, reaffirming the benefits of \gls{ml} and reflecting on lessons learned.
\end{itemize}
